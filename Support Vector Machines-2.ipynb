{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d7047-401b-4d62-ab41-7a06653fe794",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the Relationship Between Polynomial Functions and Kernel Functions in Machine Learning Algorithms?\n",
    "Polynomial functions and kernel functions are related in the context of Support Vector Machines (SVMs) and other kernel-based learning algorithms.\n",
    "Polynomial Functions:\n",
    "•\tPolynomial functions can map input data into a higher-dimensional space by including polynomial terms. For example, a polynomial function of degree 2 includes terms like x12x_1^2x12, x22x_2^2x22, x1x2x_1 x_2x1x2, etc.\n",
    "Kernel Functions:\n",
    "•\tKernel functions compute the inner product of data points in a higher-dimensional feature space without explicitly performing the transformation. They allow algorithms to operate in this higher-dimensional space implicitly.\n",
    "•\tThe polynomial kernel is one example of a kernel function that corresponds to a polynomial feature mapping. It computes the inner product of the polynomial features of the data.\n",
    "Polynomial Kernel Function: K(xi,xj)=(αxiTxj+c)dK(\\mathbf{x}_i, \\mathbf{x}_j) = (\\alpha \\mathbf{x}_i^T \\mathbf{x}_j + c)^dK(xi,xj)=(αxiTxj+c)d where:\n",
    "•\tα\\alphaα is a coefficient,\n",
    "•\tccc is a constant term,\n",
    "•\tddd is the degree of the polynomial.\n",
    "This kernel function allows SVMs to learn non-linear decision boundaries by implicitly mapping the input features to a higher-dimensional space.\n",
    "Q2. How Can We Implement an SVM with a Polynomial Kernel in Python Using Scikit-Learn?\n",
    "Here's how you can implement an SVM with a polynomial kernel using Scikit-Learn:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Convert to binary classification for simplicity (class 0 vs. class 1)\n",
    "y = np.where(y == 2, 1, 0)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the SVM with polynomial kernel\n",
    "clf_poly = SVC(kernel='poly', degree=3, C=1.0, gamma='auto')\n",
    "clf_poly.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf_poly.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "Q3. How Does Increasing the Value of Epsilon Affect the Number of Support Vectors in SVR?\n",
    "In Support Vector Regression (SVR), the epsilon (ϵ\\epsilonϵ) parameter defines the margin of tolerance where no penalty is given for errors.\n",
    "•\tIncreasing Epsilon:\n",
    "o\tA larger ϵ\\epsilonϵ means that more points are allowed to fall within the ϵ\\epsilonϵ-tube around the regression function without incurring a penalty. This generally results in fewer support vectors since more data points are considered within the acceptable margin.\n",
    "•\tDecreasing Epsilon:\n",
    "o\tA smaller ϵ\\epsilonϵ allows fewer data points to fall within the ϵ\\epsilonϵ-tube, which usually results in more support vectors as more points are penalized.\n",
    "Q4. How Does the Choice of Kernel Function, C Parameter, Epsilon Parameter, and Gamma Parameter Affect the Performance of Support Vector Regression (SVR)?\n",
    "1.\tKernel Function:\n",
    "o\tLinear Kernel: Suitable for linearly separable data. It may perform poorly for non-linear relationships.\n",
    "o\tPolynomial Kernel: Useful for polynomial relationships. The degree of the polynomial affects the complexity of the decision boundary.\n",
    "o\tRBF Kernel: Effective for non-linear data. It can capture complex relationships but requires tuning of gamma.\n",
    "2.\tC Parameter (Regularization):\n",
    "o\tHigher C: Less regularization. It tries to fit the training data as closely as possible, which can lead to overfitting.\n",
    "o\tLower C: More regularization. It allows some errors in the training data, which can help generalize better but may lead to underfitting.\n",
    "3.\tEpsilon Parameter:\n",
    "o\tHigher Epsilon: Larger margin of tolerance. This allows more data points to be within the ϵ\\epsilonϵ-tube, which can reduce the number of support vectors but may lead to underfitting.\n",
    "o\tLower Epsilon: Smaller margin of tolerance. This can increase the number of support vectors and model complexity, potentially leading to overfitting.\n",
    "4.\tGamma Parameter:\n",
    "o\tHigher Gamma: The influence of each training example is limited to a small region. This can lead to a very complex model that fits the training data closely but may not generalize well.\n",
    "o\tLower Gamma: The influence of each training example is broader. This can smooth out the decision boundary but may not capture complex patterns in the data.\n",
    "Examples of Parameter Adjustment:\n",
    "•\tHigh C and low epsilon might be suitable for data where precise fitting is critical, such as in predicting high-stakes outcomes.\n",
    "•\tLow C and high epsilon could be useful for noisy data where overfitting is a concern.\n",
    "Q5. Assignment\n",
    "Steps for Implementing an SVC Classifier:\n",
    "1.\tImport Libraries and Load Dataset:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the dataset (example: diabetes.csv)\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "2.\tSplit the Dataset:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "3.\tPreprocess the Data:\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "4.\tCreate and Train the SVC Classifier:\n",
    "clf = SVC(kernel='linear', C=1.0)  # Using linear kernel as an example\n",
    "clf.fit(X_train, y_train)\n",
    "5.\tPredict and Evaluate:\n",
    "python\n",
    "Copy code\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "6.\tHyperparameter Tuning:\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.001, 0.01, 0.1],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Retrain with the best parameters\n",
    "best_clf = grid_search.best_estimator_\n",
    "best_clf.fit(X, y)\n",
    "7.\tSave the Trained Classifier:\n",
    "import joblib\n",
    "\n",
    "joblib.dump(best_clf, 'svm_classifier.pkl')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
